from openai import OpenAI

client = OpenAI(
  base_url = "https://integrate.api.nvidia.com/v1",
  api_key = "$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC"
)

completion = client.chat.completions.create(
  model="nvidia/nemotron-4-340b-instruct",
  messages=[{"role":"user","content":"import numpy as np\nimport importlib\nimport copy\nimport math\n\ncp = importlib.import_module(\"06_createdata_plot\")\n\nNETWORK_SHAPE = [2, 100, 200, 100, 50, 2]\nBATCH_SIZE = 30\nLEARNING_RATE = 0.015\nLOSS_THRESHOLD = 0.1\nFORCE_TRAIN_THRESHOLD = 0.05\n\nforce_train = False\nrandom_train = False\nn_improved = 0\nn_not_improved = 0\ncurrent_loss = 1\n\n\n#标准化函数\ndef normalize(array):\n    max_number = np.max(np.absolute(array), axis=1, keepdims=True)\n    scale_rate = np.where(max_number == 0, 1, 1/max_number)\n    norm = array * scale_rate\n    return norm\n\n#向量标准化函数\ndef vector_normalize(array):\n    max_number = np.max(np.absolute(array))\n    scale_rate = np.where(max_number == 0, 1, 1/max_number)\n    norm = array * scale_rate\n    return norm\n\n# 激活函数\ndef activation_ReLU(inputs):\n    return np.maximum(0, inputs)\n\n#分类函数\ndef classify(probabilities):\n    classification = np.rint(probabilities[:, 1])\n    return classification\n    \n# softmax激活函数\ndef activation_softmax(inputs):\n    max_values = np.max(inputs, axis=1, keepdims=True)\n    slided_inputs = inputs - max_values\n    exp_values = np.exp(slided_inputs)\n    norm_base = np.sum(exp_values, axis=1, keepdims=True)\n    norm_values = exp_values/norm_base\n    return norm_values\n\n#损失函数1\ndef precise_loss_function(predicted, real):\n    real_matrix = np.zeros((len(real), 2))\n    real_matrix[:, 1] = real\n    real_matrix[:, 0] = 1 - real\n    product = np.sum(predicted*real_matrix, axis=1)\n    return 1 - product\n\n#损失函数2\ndef loss_function(predicted, real):\n    condition = (predicted > 0.5)\n    binary_predicted = np.where(condition, 1, 0)\n    real_matrix = np.zeros((len(real), 2))\n    real_matrix[:, 1] = real\n    real_matrix[:, 0] = 1 - real\n    product = np.sum(binary_predicted*real_matrix, axis=1)\n    return 1 - product\n\n#需求函数\ndef get_final_layer_preAct_damands(predicted_values, target_vector):\n    target = np.zeros((len(target_vector), 2))\n    target[:, 1] = target_vector\n    target[:, 0] = 1 - target_vector\n    \n    for i in range(len(target_vector)):\n        if np.dot(target[i], predicted_values[i]) > 0.5:\n            target[i] = np.array([0, 0])\n        else:\n            target[i] = (target[i] - 0.5) * 2\n    return target\n\n#-------------------------------------------------------------\n#定义一个层类\nclass Layer: \n    def __init__(self, n_inputs, n_neurons):\n        self.weights = np.random.randn(n_inputs, n_neurons)\n        self.biases = np.random.randn(n_neurons)\n    \n    def layer_forward(self, inputs):\n        self.output = np.dot(inputs, self.weights) + self.biases\n        return self.output\n    \n    def layer_backward(self, preWeights_values, afterWeights_demands):\n        preWeights_damands = np.dot(afterWeights_demands, self.weights.T)\n        \n        condition = (preWeights_values > 0)\n        value_derivatives = np.where(condition, 1, 0)\n\n        preActs_demands = value_derivatives*preWeights_damands\n        norm_preActs_demands = normalize(preActs_demands)\n        \n        weight_adjust_matrix = self.get_weight_adjust_matrix(preWeights_values, afterWeights_demands)\n        norm_weight_adjust_matrix = normalize(weight_adjust_matrix)\n        \n        return (norm_preActs_demands, norm_weight_adjust_matrix)\n    \n    def get_weight_adjust_matrix(self, preWeights_values, aftWeights_demands):\n        plain_weights = np.full(self.weights.shape, 1)\n        weights_adjust_matrix = np.full(self.weights.shape, 0.0)\n        plain_weights_T = plain_weights.T\n        \n        for i in range(BATCH_SIZE):\n            weights_adjust_matrix += (plain_weights_T*preWeights_values[i, :]).T*aftWeights_demands[i, :]\n        weights_adjust_matrix = weights_adjust_matrix/BATCH_SIZE\n        return weights_adjust_matrix\n        \n#-----------------------------------------------------------------   \n#定义一个网络类\nclass Network:\n    def __init__(self, network_shape):\n        self.shape = network_shape\n        self.layers = []\n        for i in range(len(network_shape)-1):\n            layer = Layer(network_shape[i], network_shape[i+1])\n            self.layers.append(layer)\n     \n    #前馈运算函数 \n    def network_forward(self, inputs):\n        outputs = [inputs]\n        for i in range(len(self.layers)):\n            layer_sum = self.layers[i].layer_forward(outputs[i])\n            if i < len(self.layers)-1:\n                layer_output = activation_ReLU(layer_sum)\n                layer_output = normalize(layer_output)\n            else:\n                layer_output = activation_softmax(layer_sum)\n            outputs.append(layer_output)\n        return outputs\n    \n    #反向传播函数\n    def network_backward(self, layer_outputs, target_vector):\n        backup_network = copy.deepcopy(self) # 备用网络\n        preAct_demands = get_final_layer_preAct_damands(layer_outputs[-1], target_vector)\n        for i in range(len(self.layers)):\n            layer = backup_network.layers[len(self.layers) - (1+i)] # 倒序\n            if i != 0:\n                layer.biases += LEARNING_RATE * np.mean(preAct_demands, axis=0)\n                layer.biases = vector_normalize(layer.biases)\n                \n            outputs = layer_outputs[len(layer_outputs) - (2+i)]\n            results_list = layer.layer_backward(outputs, preAct_demands)\n            preAct_demands = results_list[0]\n            weights_adjust_matrix = results_list[1]\n            layer.weights += LEARNING_RATE * weights_adjust_matrix\n            layer.weights = normalize(layer.weights)\n        return backup_network\n    \n    #单批次训练\n    def one_batch_train(self, batch):\n        global force_train, random_train, n_improved, n_not_improved\n\n        inputs = batch[:,(0, 1)]\n        targets = copy.deepcopy(batch[:, 2]).astype(int) # 标准答案\n        outputs = self.network_forward(inputs)\n        precise_loss = precise_loss_function(outputs[-1], targets)\n        loss = loss_function(outputs[-1], targets)\n            \n        if np.mean(loss) <= LOSS_THRESHOLD:#损失函数小于这个值就不需要训练了\n            print('No need for training')\n        else:\n            backup_network = self.network_backward(outputs, targets)\n            backup_outputs = backup_network.network_forward(inputs)\n            backup_precise_loss = precise_loss_function(backup_outputs[-1], targets)\n            backup_loss = loss_function(backup_outputs[-1], targets)\n            \n            if np.mean(precise_loss) >= np.mean(backup_precise_loss) or np.mean(loss) >= np.mean(backup_loss):\n                for i in range(len(self.layers)):\n                    self.layers[i].weights = backup_network.layers[i].weights.copy()\n                    self.layers[i].biases = backup_network.layers[i].biases.copy()\n                print('Improved')\n                n_improved += 1\n            \n            else:\n                if force_train:\n                    for i in range(len(self.layers)):\n                        self.layers[i].weights = backup_network.layers[i].weights.copy()\n                        self.layers[i].biases = backup_network.layers[i].biases.copy()\n                    print('Force train')\n                if random_train:\n                    self.random_update()\n                    print(\"Random update\")\n                else:  \n                    print('No improvement')\n                n_not_improved += 1\n        print('-----------------------------------------')\n            \n    #多批次训练\n    def train(self, n_entries):\n        global force_train, random_train, n_improved, n_not_improved\n        n_improved = 0\n        n_not_improved = 0\n\n        n_batches = math.ceil(n_entries/BATCH_SIZE)\n        for i in range(n_batches):\n            batch = cp.creat_data(BATCH_SIZE)\n            self.one_batch_train(batch)\n        \n        # 添加检查，避免除以零的错误\n        if n_improved + n_not_improved > 0:\n            improvement_rate = n_improved / (n_improved + n_not_improved)\n        else:\n            improvement_rate = 0  # 或者其他适当的默认值\n\n        print(\"改进率\")\n        print(format(improvement_rate, \".0%\"))\n        \n        if improvement_rate <= FORCE_TRAIN_THRESHOLD:\n            force_train = True\n        else:\n            force_train = False\n        if n_improved == 0:\n            random_train = True\n        else:\n            random_train = False\n        \n        data = cp.creat_data(800)\n        inputs = data[:, (0, 1)]\n        outputs = self.network_forward(inputs)\n        classification = classify(outputs[-1])\n        data[:, 2] = classification\n        cp.plot_data(data, \"After training\")\n        \n    #随机更新\n    def random_update(self):\n        random_network = Network(NETWORK_SHAPE)\n        for i in range(len(self.layers)):\n            weights_change = random_network.layers[i].weights\n            biases_change = random_network.layers[i].biases\n            self.layers[i].weights += weights_change\n            self.layers[i].biases += biases_change\n            \n    #核验LOSS值\n    def check_loss(self):\n        data = cp.creat_data(1000)\n        inputs = data[:, (0, 1)]\n        targets = copy.deepcopy(data[:, 2]).astype(int) # 标准答案\n        outputs = self.network_forward(inputs)\n        loss = loss_function(outputs[-1], targets)\n        return np.mean(loss)\n        \n#-------------MAIN-------------------------\ndef main():\n    global current_loss\n    data = cp.creat_data(800) #生成数据\n    cp.plot_data(data, \"Right classification\")\n\n    #选择起始网络\n    use_this_network = 'n' #No\n    while use_this_network != 'Y' and use_this_network != 'y':\n        network = Network(NETWORK_SHAPE)\n        inputs = data[:, (0, 1)]\n        outputs = network.network_forward(inputs)\n        classification = classify(outputs[-1])\n        data[:, 2] = classification\n        cp.plot_data(data, \"Choose network\")\n        use_this_network = input(\"Use this network? Y to yes, N to No \\n\")\n    \n    #进行训练\n    do_train = input(\"Train? Y to yes, N to No \\n\")\n    while do_train == 'Y' or do_train == 'y' or do_train.isnumeric() == True:\n        if do_train.isnumeric() == True:\n            n_entries = int(do_train)\n        else:\n            n_entries = int(input(\"Enter the number of data entries used to train. \\n\"))\n            \n        network.train(n_entries)\n        do_train = input(\"Train? Y to yes, N to No \\n\")\n        \n    #演示训练效果\n    inputs = data[:, (0, 1)]\n    outputs = network.network_forward(inputs)\n    classification = classify(outputs[-1])\n    data[:, 2] = classification\n    cp.plot_data(data, \"After training\")\n    print(\"谢谢，再见！\")\n#----------------TEST-------------------------\ndef test():\n    pass\n\n#--------------运行---------------------\nmain()\n\nASCII字符来表示下我的这个代码中的链式法则\n\n为了用ASCII字符表示你的代码中的链式法则，我们可以使用简单的箭头和数学符号来表示。以下是Layer类的layer_backward方法中链式法则的ASCII表示：\n\npreWeights_values (x) ---> layer_sum (z) ---> afterWeights_demands (y)\n\n                         ReLU activation\n\ndy/dx = (dy/dz) * (dz/dx)\n\n(dy/dz) = afterWeights_demands\n\n(dz/dx) = value_derivatives = np.where(preWeights_values > 0, 1, 0)\n\npreWeights_damands = np.dot(afterWeights_demands, self.weights.T)\n\npreActs_demands = value_derivatives * preWeights_damands\n\nnorm_preActs_demands = normalize(preActs_demands)\n\n在这个ASCII图中，preWeights_values表示为x，layer_sum表示为z，afterWeights_demands表示为y。箭头表示数据流向，ReLU激活函数应用于layer_sum（z）上。链式法则的应用表示为dy/dx = (dy/dz) * (dz/dx)，其中(dy/dz)表示为afterWeights_demands，(dz/dx)表示为value_derivatives。然后，我们计算preWeights_damands，并标准化为norm_preActs_demands。\n\n上边的描述确认吗？"}],
  temperature=0.2,
  top_p=0.7,
  max_tokens=10000,
  stream=True
)

for chunk in completion:
  if chunk.choices[0].delta.content is not None:
    print(chunk.choices[0].delta.content, end="")

